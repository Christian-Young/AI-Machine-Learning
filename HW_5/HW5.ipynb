{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlMF0ejlZu2k2/1MTOyAg3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Christian-Young/AI-Machine-Learning/blob/master/HW_5/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Li03T-lO7mb",
        "colab_type": "text"
      },
      "source": [
        "Summarize and describe the different concepts/methods/algorithms that you have learned in this course.\n",
        "\n",
        "Use a Colab notebook. Make sure that you organize the material logically by using sections/subsections. Also, use code cell to include code snippets.\n",
        "\n",
        "I suggest that you group everything into five categories:\n",
        "\n",
        "* General concepts (for instance, what is artificial intelligence, machine learning, deep learning)\n",
        "\n",
        "* Basic concepts (for instance, here you can talk about linear regression, logistic regression, gradients, gradient descent)\n",
        "\n",
        "* Building a model (for instance, here you can talk about the structure of a convent, what it components are etc.)\n",
        "\n",
        "* Comping a model (for instance, you can talk here about optimizers, learning rate etc.)\n",
        "\n",
        "* Training a model (for instance, you can talk about overfitting/underfitting)\n",
        "\n",
        "* Finetuning a pretrained model (describe how you proceed)\n",
        "\n",
        "Take this homework very seriously. You have the opportunity to make up for lost point on previous homework assignments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEVjqwtuPAzT",
        "colab_type": "text"
      },
      "source": [
        "# General Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEeo5WLS4PMa",
        "colab_type": "text"
      },
      "source": [
        "### Artificial Intelligence (AI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k025bYPfPHgf",
        "colab_type": "text"
      },
      "source": [
        "In this course, I have examined the broad field of Artificial Intelligence as well as the subsets which are included within it. Artificial Intelligence (AI) is loosely defined. As such, there are several different definitions that are equally valid in representing its goals and properties. One definition is described as the \"Science and Engineering of making intelligent machines\". As mentioned by John McCarthy. Other definitions Include: \"The branch of Computer Science dealing with the simulation of intelligent behavior in computers\", among others. As for my interpretation of these definitions based on what I've learned, I would describe the broad field of AI as the construction of algorithms to perform tasks that normally would need to be done by a human."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR0tYeKv4jL_",
        "colab_type": "text"
      },
      "source": [
        "### Machine Learning (ML)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvZFRWu34dRq",
        "colab_type": "text"
      },
      "source": [
        "Machine learning (ML) is considered to be subset of AI. It is the branch of AI that deals with the study of computers being given the ability to learn with only the parameters available to them. In turn, a program with a foundation involving Machine Learning is able to adapt and respond to the results of data given to them by the programmer. With this ability, computers are able to learn dynamically without changes issued by a human. The main and most important difference between AI and ML, is that in AI, the rules are given to the computer to achieve a specified output intended by the programmer. However, in ML, the computer creates the rules themselves in response to data. This it the single most distinguishing feature between AI and ML.\n",
        "\n",
        "In Machine Learning, terminologies are used to describe the problem and the method of approaching it.\n",
        "\n",
        "* A **label** is the intended prediction.\n",
        "\n",
        "* A **feature** is an input variable which is used to predict the model.\n",
        "\n",
        "* A **bias** is often considered the \"y-intercept\".\n",
        "\n",
        "* A **weight** is essentially the same concept as \"slope\" in referring to the graphing of a model.\n",
        "\n",
        "* An **inference** is a prediction based on an input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n45mh0re4mAL",
        "colab_type": "text"
      },
      "source": [
        "### Subsets of ML: Deep Learning (DL), Reinforcement Learning (RL), Supervised/Unsupervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeqWPNha4iYc",
        "colab_type": "text"
      },
      "source": [
        "There are subsets or branches of Machine Learning as well. Such as Supervised or Unsupervised Learning. As well as Reinforcement Learning (RL) or Deep Learning (DL). Each of which have their own properties and have uses in different problems. Reinforcement Learning for instance has its main goal in adjusting a computers behavior with rewards at each step of an action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9dURVxOPQqf",
        "colab_type": "text"
      },
      "source": [
        "# Basic Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMUNAEfkPWpL",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bsuuuL660Nz",
        "colab_type": "text"
      },
      "source": [
        "In terms of Machine Learning, Linear Regression is a method of modeling a relationship between a response (such as a predicted label) and an input (a feature). A regression model relies on features (inputs) to predict a certain value. The more features, the more accurate the model. If the model has a smaller number of features, it may not represent the true nature of the trend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zq8XLFv61K_",
        "colab_type": "text"
      },
      "source": [
        "### Training and Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO-Qricf63sA",
        "colab_type": "text"
      },
      "source": [
        "**Loss** is terminology in Machine Learning important in assessing the results of training. Loss is essentially the result of a failed prediction. The purpose of loss is to display the severity of a bad prediction that a model created. Only when a prediction is perfect, is the loss equal to 0. **Training** the model is the process of adjusting variables (weights and bias) based on features and outputs. The main goal of training is to minimize loss for a more accurate model. This entire process is described as another term called **Empirical Risk Minimization**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOlP4wUl7Dvw",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaQT6rnt7GX-",
        "colab_type": "text"
      },
      "source": [
        "Gradient Descent is an iterative optimization algorithm. Its purpose is to optimize by finding a local minimum of a differentiable function. This is done by assigning a starting point on a curve of a graph of a model whose parameters are loss (for the y-axis) and the value of weights (for the x-axis). We achieve the smallest amount of loss by stepping in one direction along the curve and assessing whether we are \"warmer\" or \"colder\". Based on the result, the starting point is updated. The gradient is the variable used to make the steps in either direction. It is a vector. Meaning it has the properties of direction and magnitude. This is used to optimize the loss of the model. The \"step\" is also known as the learning rate. It is a scalar quantity that is multplied by the gradient vector. It is important to adjust the learning rate. If the learning rate is too small, it will take too long. If the rate is too large, the model may overshoot the local minimum in relation to the loss.\n",
        "\n",
        "Below is a code snippet featuring gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fik0aSgjH_hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Random data generation.\n",
        "x = 9 * np.random.rand(100, 1)\n",
        "y = 3 * np.random.rand(100, 1)\n",
        "\n",
        "# Parameters. Epochs, learning rate (step size), batch size, weights.\n",
        "epochs = 50\n",
        "lr = 0.01\n",
        "batch = 5\n",
        "wt = np.random.randn(3, 1)\n",
        "weightPath = []\n",
        "weightPath.append(wt)\n",
        "arr = np.column_stack([np.ones((100, 1)), x, y])\n",
        "\n",
        "# Gradient Descent training.\n",
        "for epoch in range(epochs):\n",
        "  indices = np.random.permutation(100)\n",
        "  xS = arr[indices]\n",
        "  yS = y[indices]\n",
        "  for i in range(0, 100, batch):\n",
        "    x_i = xS[i: i + batch]\n",
        "    y_i = yS[i: i + batch]\n",
        "    batchDot = batch * x_i.T.dot(x_i.dot(wt) - y_i)\n",
        "    gradient = 1 / batchDot\n",
        "    wt = wt - (lr * gradient)\n",
        "    weightPath.append(wt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzfUfVumPXqK",
        "colab_type": "text"
      },
      "source": [
        "# Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sfgHRUmPDQa",
        "colab_type": "text"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0OMe-bSPHTH",
        "colab_type": "text"
      },
      "source": [
        "The groundwork for a neural network is a collection of data-processing modules known as **layers**. The layers recieve input tensors from layers and output tensors as well to other layers. Layers often have a state known as a **weight**. All of which are learned from **Stochastic Gradient Descent (SGD)**. It is important to choose the correct layers for the appropriate formats or data processing.\n",
        "\n",
        "The programmer must also specify a **loss function** (objective) as well as an **optimizer** when designing the neural network architecture. The loss function is the representative of success for the current step. While the optimizer describes how the neural network must change based on the loss. Implemented by Stochastic Gradient Descent. The loss function selected is important as different functions serve different purposes for different problems.\n",
        "\n",
        "Below is a code snippet featuring the construction of a model with different layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k8m5ig-Smr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.applications import Xception\n",
        "\n",
        "# ConvNet base\n",
        "conv_base = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))\n",
        "\n",
        "# Layers\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0wyrGVpPbsw",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional Neural Networks (ConvNets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCyhPf7LNwoI",
        "colab_type": "text"
      },
      "source": [
        "Convolutional Neural Networks are instantiated with the introduction of **convolutional layers** in the model. The convolutional layers consist of several different components:\n",
        "\n",
        "* Filters\n",
        "* Kernel Size\n",
        "* Strides\n",
        "* Padding\n",
        "* Data Format\n",
        "* Dilation Rate\n",
        "* Activation\n",
        "* Use Bias\n",
        "* Kernel Initializer\n",
        "* Bias Initializer\n",
        "* Kernel Regularizer\n",
        "* Bias Regularizer\n",
        "* Activity Regularizer\n",
        "* Kernel Constraint\n",
        "* Bias Constraint\n",
        "\n",
        "Below is a code snippet featuring a convolutional base:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNt2tdwEUL6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDG1T2jgY5uA",
        "colab_type": "text"
      },
      "source": [
        "# Compiling a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0ALuBAYZIli",
        "colab_type": "text"
      },
      "source": [
        "### Optimizers and Loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzWQ7GZPZKoi",
        "colab_type": "text"
      },
      "source": [
        "The optimizer is a component of the network architecture in which the programmer has defined the layers. The optimizers purpose is to update the network based on the loss function (which the programmer also specifies). The optimizer often takes certain arguments such as learning rate, momentum (Accelerates SGD in a direction and dampens oscillations), and nesterov momentum.\n",
        "\n",
        "The loss function selected is important as different functions serve different purposes for different problems. For instance, in the case of Binary classification or if there is multi-class and multi-label classification, it is most appropriate to use the Binary Cross Entropy loss function. Another case to consider is the case that regression evaluates to arbitrary values. In this case the Mean Squared Error loss function is most appropriate. As demonstrated in the code below.\n",
        "\n",
        "Below is a snippet featuring the construction of a keras model with an optimizer and loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Vz2zRSZ5B-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# SGD optimizer.\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='mean_squared_error', optimizer=sgd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39vAkEpkPcTB",
        "colab_type": "text"
      },
      "source": [
        "# Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRBKwbtLNqe2",
        "colab_type": "text"
      },
      "source": [
        "### Overfitting/Underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coYQXzpyZGYJ",
        "colab_type": "text"
      },
      "source": [
        "When a Machine Learning model overcompensates the peculiarities of initial data, it may not be prepared for new data and it will **overfit** as a result. While a model that overfits has a low loss rate with training data, it will suffer when new data is introduced. The cause of this is attributed to an unnecessarily complex model. So one of the main downfalls of Machine Learning is to find an appropriate balance between high accuracy and simplicity with our models. The Machine Learning model needs to make good predictions of unknown data as well. Otherwise the results will end up being wildly inaccurate or outright incorrect. To alleviate overfitting. The programmer should divide the data into test and training sets. However, this method is only sound when considering the following:\n",
        "\n",
        "* Examples are **independent** from each other\n",
        "* **Stationary** distribution (i.e. the distribution does not change)\n",
        "* **Same** distribution\n",
        "\n",
        "With this strategy, a model will be able to train on the training set, evaluate on the test set, and apply updates based on the result of the test set. After each cycle of the workflow we pick the model that performs the most optimally on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw_AlT32Pg2r",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbaSu5T3Pn4z",
        "colab_type": "text"
      },
      "source": [
        "### Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q2FOQ-2ghSi",
        "colab_type": "text"
      },
      "source": [
        "As mentioned in the overfitting/underfitting section. A Machine Learning model can overestimate its predictons based on test data. And as such may not be prepared for new unforeseen data. The main solution is to partition the data into test and training sets. Which is a useful method in that it allows the model to train on some examples and test on an entirely different set of examples. However, this method is not perfect. Since partitioning the data into two sets provided more optimal results, partitioning into three different sets would provide an even higher accurancy. Then the validation set is introducted. The validation set will essentially be the intermediary stage in between the training and test sets. The purpose of the validation set is to evaluate results from the training set to allow for the test set to \"double-check\" the results of the validation set. With the data set partitioned into three, the workflow is similiar to the previous strategy of two partitions: Train model on training set, evaluate the model on the validation set, update parameters based on the validation set results, select the model that performs the most optimally on the validation set, and finally, confirm the model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN9S7qMuyIK8",
        "colab_type": "text"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWa8GJZHyK7b",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   [Slides by Dr. Pawel Wocjan](https://github.com/schneider128k/machine_learning_course/tree/master/slides)\n",
        "\n",
        "* [Optimizers](https://keras.io/optimizers/)\n",
        "\n",
        "* [Loss](https://keras.io/losses/)\n",
        "\n",
        "* [Convolutional](https://keras.io/layers/convolutional/)"
      ]
    }
  ]
}